{
  "brand_name": "JUNON LEE",
  "brand_title": "Systems & AI Researcher",

  "nav_home": "Home",
  "nav_resume": "Resume",
  "nav_portfolio": "Portfolio",
  "nav_contact": "Contact",

  "profile_name": "JUN-HEON LEE",
  "profile_title": " GPU Architecture / On-device AI",
  "profile_summary": "I engineer hardware-conscious ML systems that stay lean yet scale to production workloads.",

  "sidebar_profile_heading": "Personal profile",
  "sidebar_profile_body": "I am a researcher working on on-device AI and GPU memory systems. My work focuses on developing structural optimization techniques that enable large-scale models to run reliably even under limited hardware resources.",
  "sidebar_contact_heading": "Contact",
  "sidebar_contact_email_label": "Email",
  "sidebar_contact_email_value": "wnsgjs34@gmail.com",
  "sidebar_contact_phone_label": "Phone",
  "sidebar_contact_phone_value": "+82-10-7182-9744",
  "sidebar_contact_location_label": "Location",
  "sidebar_contact_location_value": "Seoul, South Korea",
  "sidebar_contact_site_label": "Website",
  "sidebar_contact_site_value": "junon-lee.pages.dev",
  "sidebar_education_heading": "Education",
  "sidebar_education_one": "University of Seoul · M.S. Electrical and Computer Engineering",
  "sidebar_education_one_sub": "2025 - Present",
  "sidebar_education_two": "University of Seoul · B.S. Electrical and Computer Engineering",
  "sidebar_education_two_sub": "2019 - 2025",
  "sidebar_skills_heading": "Skills",
  "sidebar_skill_one": "On-device AI",
  "sidebar_skill_two": "GPU virtualization",
  "sidebar_skill_three": "Mixture-of-experts",
  "sidebar_skill_four": "Profiling & tooling",
  "sidebar_skill_five": "Technical writing",
  "sidebar_projects_heading": "Highlighted projects",
  "sidebar_project_one": "Edge Profiler",
  "sidebar_project_one_sub": "Scheduling + telemetry stack",
  "sidebar_project_two": "KV Cache Compressor",
  "sidebar_project_two_sub": "Latency-aware compression",

  "intro_heading": "Spec sheet overview",
  "intro_body": "The portfolio is anchored by this page. Every block below links to a more detailed project or publication entry.",
  "intro_cta_resume": "Open resume",
  "intro_cta_portfolio": "View portfolio",
  "intro_cta_contact": "Drop a note",

  "home_featured": "Featured work",
  "home_intro_desc": "Filter between engineering deliverables and research write-ups, then click a card to explore the case study.",
  "filter_all": "All",
  "filter_projects": "Projects",
  "filter_research": "Research",

  "work_esmoe_title": "ES-MoE inference optimizer",
  "work_esmoe_sub": "Sparse expert routing runtime",
  "work_esmoe_date": "2025.05",
  "work_orion_title": "Project Orion edge stack",
  "work_orion_sub": "Telemetry + remediation pipeline",
  "work_orion_date": "2025.03",
  "work_llm_title": "Efficient LLM offloading",
  "work_llm_sub": "Journal-style findings",
  "work_llm_date": "2024.11",
  "work_cxl_title": "CXL-aware tiered memory",
  "work_cxl_sub": "Multi-tier placement policy",
  "work_cxl_date": "2024.08",

  "portfolio_intro_title": "Portfolio overview",
  "portfolio_intro_desc": "This page showcases the research and projects I have worked on.",
  "portfolio_projects_title": "Engineering projects",
  "portfolio_projects_desc": "Each tile routes to a project detail page with research-worthy copy.",
  "portfolio_research_title": "Research & publications",
  "portfolio_research_desc": "Summaries of work and notes from recent experiments.",

  "resume_intro_title": "Resume snapshot",
  "resume_intro_desc": "This page lists the hands-on experience, publications, and awards that kept the home hero concise.",
  "resume_experience_title": "Experience",
  "resume_exp1_role": "Graduate researcher · ACAS Lab",
  "resume_exp1_period": "2025 - Present",
  "resume_exp1_desc": "Building inference optimizers and runtime automation for hybrid GPU/PIM clusters.",
  "resume_exp2_role": "Systems engineer intern · FlexGen",
  "resume_exp2_period": "Summer 2024",
  "resume_exp2_desc": "Implemented KV cache scheduling kernels and benchmark automation on Blackwell hardware.",
  "resume_exp3_role": "Teaching assistant · Systems programming",
  "resume_exp3_period": "2024",
  "resume_exp3_desc": "Led labs covering Verilog, GPU simulation, and OS instrumentation for 40+ students.",
  "resume_exp4_role": "Airforce, Republic of Korea",
  "resume_exp4_period": "Aug 2019 - May 2021",
  "resume_exp4_desc": "Staff sergeant/ honorable discharge",
  "resume_research_title": "Research highlights",
  "resume_pub1_title": "ES-MoE: Expert scaling for edge inference",
  "resume_pub1_meta": "Under review · First author",
  "resume_pub2_title": "Memory-aware LLM offloading",
  "resume_pub2_meta": "MLSys 2024 · Co-author",
  "resume_pub3_title": "CXL-aware tiering policies",
  "resume_pub3_meta": "HPCA 2023 · Co-author",
  "resume_projects_title": "Selected builds",
  "resume_proj1_title": "Project Orion telemetry pipeline",
  "resume_proj1_desc": "Fault-tolerant synchronization and real-time health scoring for fleets.",
  "resume_proj2_title": "FlexBench profiling suite",
  "resume_proj2_desc": "Open-source tooling to capture latency and power for each LLM component.",
  "resume_awards_title": "Awards & recognition",
  "resume_award1_title": "KAIST presidential scholarship",
  "resume_award1_desc": "Full scholarship for top 1% researchers",
  "resume_award2_title": "Outstanding teaching assistant",
  "resume_award2_desc": "System Programming · Spring 2024",

  "contact_intro_title": "Contact guide",
  "contact_intro_desc": "Share a quick note or book a visit. Below are the doors that stay open.",
  "contact_card_lab_title": "Lab visits",
  "contact_card_lab_desc": "Room 613, IT Building, University of Seoul, Seoul",
  "contact_card_talks_title": "TA / Lab Sessions",
  "contact_card_talks_desc": "Please contact me via email or phone.",
  "contact_card_collab_title": "Collaboration",
  "contact_card_collab_desc": "Open to joint research in related fields.",
  "contact_card_career_title": "Recruiting",
  "contact_card_career_desc": "Open to R&D opportunities in AI systems, GPU architecture, and system optimization.",
  "contact_form_title": "Quick message",
  "contact_form_desc": "Drop the topic you want to chat about and I will reply soon.",
  "contact_form_name": "Your name",
  "contact_form_email": "Work email",
  "contact_form_message": "Reason for writing",
  "contact_form_cta": "Send message",

  "detail_label_role": "Role",
  "detail_label_timeline": "Timeline",
  "detail_label_stack": "Stack",
  "detail_overview_heading": "Overview",
  "detail_contribution_heading": "Key contributions",
  "detail_findings_heading": "Research findings",
  "detail_outcome_heading": "Impact",
  "detail_btn_visit": "Visit site",
  "detail_btn_deck": "Download deck",
  "detail_btn_paper": "Read paper",
  "detail_btn_slides": "View slides",

  "detail_esmoe_title": "ES-MoE inference optimizer",
  "detail_esmoe_subtitle": "Expert-scaling toolkit for edge GPUs",
  "detail_esmoe_role": "Lead researcher",
  "detail_esmoe_timeline": "2024 - 2025",
  "detail_esmoe_stack": "PyTorch, CUDA Graphs, Triton",
  "detail_esmoe_overview": "Optimized sparse MoE routing with latency-aware placement and instrumentation for small GPU pods.",
  "detail_esmoe_point1": "Designed a 40µs expert selection that respects device bandwidth.",
  "detail_esmoe_point2": "Implemented streaming KV cache compression with 2× throughput vs. PyTorch baseline.",
  "detail_esmoe_point3": "Delivered dashboards showing per-expert utilization across deployments.",
  "detail_esmoe_outcome": "Reduced end-to-end latency by 38% on ACAS Lab robots.",

  "detail_orion_title": "Project Orion edge stack",
  "detail_orion_subtitle": "Telemetry + resilience for autonomous drones",
  "detail_orion_role": "Systems engineer",
  "detail_orion_timeline": "2023 - 2024",
  "detail_orion_stack": "Rust, gRPC, Prometheus, Azure IoT",
  "detail_orion_overview": "Built reliable pipelines syncing GPU metrics and mission data even over flaky links.",
  "detail_orion_point1": "Shipped adaptive buffering surviving 15-minute disconnects.",
  "detail_orion_point2": "Exposed GPU thermal throttling through profiler hooks.",
  "detail_orion_point3": "Packaged mission replay dashboards for field teams.",
  "detail_orion_outcome": "Cut support escalations by 45% and enabled 12 pilot programs.",

  "detail_llm_title": "Efficient LLM offloading",
  "detail_llm_subtitle": "Pipeline scheduling for mobile GPUs",
  "detail_llm_role": "First author",
  "detail_llm_timeline": "2024",
  "detail_llm_stack": "CUDA, PyTorch, FlexGen, TensorRT",
  "detail_llm_overview": "Quantifies hybrid CPU/GPU execution that preserves large sequences inside memory budgets.",
  "detail_llm_point1": "Derived overlapping prefill/decode models under bandwidth limits.",
  "detail_llm_point2": "Introduced progressive cache eviction within 6 GB VRAM.",
  "detail_llm_point3": "Benchmarked on Jetson Orin with 1.6× throughput gains.",
  "detail_llm_outcome": "Spotlight talk; widely cited for mobile LLM deployments.",

  "detail_cxl_title": "CXL-aware tiered memory",
  "detail_cxl_subtitle": "Dynamic placement for HBM + CXL clusters",
  "detail_cxl_role": "Co-author",
  "detail_cxl_timeline": "2023",
  "detail_cxl_stack": "CXL simulator, Python, NVProf",
  "detail_cxl_overview": "Modeled multi-tier nodes with CXL expanders and explored cache migration heuristics.",
  "detail_cxl_point1": "Proposed latency predictors for spilling tensors to CXL pools.",
  "detail_cxl_point2": "Implemented a feedback controller reacting to queueing spikes.",
  "detail_cxl_point3": "Achieved 27% better cost-per-query by mixing CXL and HBM.",
  "detail_cxl_outcome": "Influenced hardware roadmaps with industry partners.",

  "footer_text": "(C) 2025 Your Name. All Rights Reserved."
}
